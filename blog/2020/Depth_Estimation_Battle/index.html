<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Depth Estimation Battle | Samuel Pietri</title> <meta name="author" content="Samuel Pietri"/> <meta name="description" content="Comparison between different depth estimation techniques"/> <meta name="keywords" content="machine learning, deep learning, generative art, creative coding, creative technologist, technical artist"/> <meta property="og:site_name" content="Samuel Pietri"/> <meta property="og:type" content="website"/> <meta property="og:title" content="Samuel Pietri | Depth Estimation Battle"/> <meta property="og:url" content="https://samuelpietri.github.io/blog/2020/Depth_Estimation_Battle/"/> <meta property="og:description" content="Comparison between different depth estimation techniques"/> <meta property="og:locale" content="en"/> <meta name="twitter:card" content="summary"/> <meta name="twitter:title" content="Depth Estimation Battle"/> <meta name="twitter:description" content="Comparison between different depth estimation techniques"/> <meta name="twitter:site" content="@samuelpietri"/> <meta name="twitter:creator" content="@samuelpietri"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://samuelpietri.github.io/blog/2020/Depth_Estimation_Battle/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body class="fixed-top-navsticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Samuel </span>Pietri</a> <div class="navbar-brand social"> <a href="mailto:%70%69%65%74%72%69.%73%61%6D%75%65%6C@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://github.com/samuelpietri" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/samuel-pietri" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/samuelpietri" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> <a href="https://instagram.com/samuelpietri" title="Instagram" target="_blank" rel="noopener noreferrer"><i class="fab fa-instagram"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/projects/">PROJECTS</a> </li> <li class="nav-item "> <a class="nav-link" href="/">ABOUT</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Depth Estimation Battle</h1> <p>Comparison between different depth estimation techniques</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#processing-still-images">Processing still images</a></div> <div><a href="#processing-video-footage">Processing video footage</a></div> </nav> </d-contents> <figure> <img src="/assets/img/posts/depth_estimation/video_dancer_depth.gif"> <figcaption>CVDE monocular depth estimation</figcaption> </figure> <p>It’s been a while since I wanted to test different depth estimation models one against each other. Each one is mainly employing deep neural networks as a backbone algorithm for the extrapolation of the depth map.</p> <p>I’ve been working with different implementations for a set of projects at <a href="https://www.fuseworks.it/en/" target="_blank" rel="noopener noreferrer">fuse*</a>, the studio in which I’m currently working, but I’ve never really made an extensive side by side evaluation. So I’ve collected a few interesting models to test. As everybody knows the Deep Learning field is blossoming and the rate at which new projects are coming out is outrageous, so I’ll probably miss some nice research ones that fit into this category, in that case I’ll probably test them out later.</p> <h2 id="processing-still-images">Processing still images</h2> <p>First I did some tests on single images taking into consideration three models:</p> <ul> <li> <a href="https://github.com/sniklaus/3d-ken-burns" target="_blank" rel="noopener noreferrer"><strong>3D Ken Burns</strong></a> - <em>3D Ken Burns Effect from a Single Image using PyTorch</em> </li> <li> <a href="https://github.com/vt-vl-lab/3d-photo-inpainting" target="_blank" rel="noopener noreferrer"><strong>3D Photo Inpainting</strong></a> - <em>3D Photography using Context-aware Layered Depth Inpainting</em> </li> <li> <a href="https://github.com/intel-isl/MiDaS" target="_blank" rel="noopener noreferrer"><strong>MiDaS</strong></a> - <em>Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer</em> </li> </ul> <p>To be fair the 3D Photo Inpainting model uses the first version of MiDaS, so in reality I’m evaluating the 3D Ken Burns depth estimation tool and three different iterations of the MiDaS project, with the last update coming in November 2020 and a refined model trained on 10 different datasets.</p> <p>I won’t get into the technical details and differences between each of the architecture used by these research group but from these simple examples it gets interesting to see how they react and perform.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/depth_estimation/depth_comparison.jpg" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Most of them have been trained on images from street dataset (depth estimation is extremely useful for self-driving car applications) so it seems that generally the buildings are really helping the model to understand better what is going on. When it comes to the human figure of the dancer this are getting a little trickier. The first two models don’t perform very well, and the retrieved depth map of the dancer is getting blurry or totally incorrect in the outer part of the body while the last two seems to have better extracted the outline of the figure from the background environment.</p> <p>Below, the same models have been applied to a diverse set of images. The depth estimator used for by the 3D Ken Burns project is clearly failing at most of the tasks but the others area clearly doing a good job, given the diversity of the content they’ve been facing. While clearly the smaller MiDaS model is compromising a bit of the accuracy due to the reduced size of the models if process the images way faster, the two last models on the right are the best ones with the v1.0 that seems to grasp more details but it also create some artifacts or cloudy areas where something weird is clearly going on.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/depth_estimation/depth_comparison2.jpg" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>As you may have guessed from the names of some of these projects some of the applications revolve around the idea of animating still photos by informing the pixels of their depth information. Even though the depth map is not perfectly matching and the network if forced to guess what is happening in any occuluded part of the image interesting effects can emerge. As shown by these two examples below virtual camera animation can be implemented to give an interesting 3-dimentional look to the picture.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/depth_estimation/swing.gif" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/depth_estimation/dolly_zoom_in.gif" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="processing-video-footage">Processing video footage</h2> <p>Images are fine but what about videos. I took the most promising model from the previous test (MiDaS v2.1) and put it head to head with another compelling model developed by the <a href="https://opensource.fb.com/" target="_blank" rel="noopener noreferrer">Facebook Research</a> group called <strong><a href="https://github.com/facebookresearch/consistent_depth" target="_blank" rel="noopener noreferrer">Consistent Video Depth Estimation</a>.</strong> Unlike the the models used before this one is specifically developed to deal with continuous stream of images and the algorithm should reconstruct dense, geometrically consistent depth for all pixels in the video. At test time the network is fine-tuned to satisfy geometric constraints achieving higher accuracy and higher degree of geometric consistency.</p> <p>The real big drawback, though, is timing. With this video 4 seconds long, the whole process took 90 minutes to perform on a decent gpu so it gets difficult to try out things on the fly (which is my jam). To optimize this process images are also resized so most of the details are lost even before any computation occurs.</p> <p>The process starts with a monocular depth estimation, and it looks ok. Doesn’t seem to suffer from any pariticular problem.</p> <figure> <img src="/assets/img/posts/depth_estimation/cvde_process.jpg"> <figcaption>CVDE Process</figcaption> </figure> <p>When the additional process kicks in, the background seems much more stable and without artifacts but the dancer figures suddenly are completely miscalculated. It probably needs some more testing to really figure out what is actually causing the incorrect estimation . I would say that the camera motion occuring at some point of the footage and the reflections on the floor are not helping.</p> <figure> <img src="/assets/img/posts/depth_estimation/color_consistent_mc.gif"> <figcaption>CVDE monocular depth estimation</figcaption> </figure> <figure> <img src="/assets/img/posts/depth_estimation/color_consistent_full.gif"> <figcaption>CVDE monocular depth estimation with additional consistency check</figcaption> </figure> <figure> <img src="/assets/img/posts/depth_estimation/color_midas.gif"> <figcaption>MiDaS v2.1 depth estimation</figcaption> </figure> <p>Overall the MiDaS 2.1 version looks better to me. A lot of flickering over the whole length of the clip but no really evident errors are appearing and</p> <p>I’ll definitely do some more extensive testing on videos, trying the models on a diverse set of content and increasing the amount of details extracted. Another interesting application would be to have a real time depth-estimation camera application creating an interesting competition with IR cameras.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Samuel Pietri. </div> </footer> <d-bibliography src="/assets/bibliography/"></d-bibliography> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>